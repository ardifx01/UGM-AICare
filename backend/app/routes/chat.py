# backend/app/routes/chat.py

from sqlalchemy.orm import Session # Import Session
import hashlib # Import hashlib
from fastapi import APIRouter, HTTPException, Body, Depends, status # type: ignore # Added Depends
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Optional, Literal

# Adjust import based on your project structure
from app.database import get_db
from app.models import User, Conversation # Import User and Conversation models
from app.core import llm
from app.core.llm import LLMProvider # Import the type hint

# Import your authentication dependency if you have one
# from app.dependencies import get_current_user # Example

router = APIRouter()

# --- Request Body Model ---
class ChatRequest(BaseModel):
    user_identifier: str = Field(..., description="Unique identifier for the user (e.g., email, username, or frontend-generated ID). Will be hashed.")
    session_id: str = Field(..., description="Unique identifier for this specific conversation session (e.g., UUID generated by frontend).")
    history: List[Dict[str, str]] = Field(..., description="Conversation history, ending with the user's prompt")
    provider: LLMProvider = Field(
        default="togetherai",
        description="The LLM provider to use ('togetherai' or 'gemini')"
    )
    model: Optional[str] = Field(
        default=None,
        description="Optional: Specify a model name override"
    )
    max_tokens: int = Field(
        default=1024,
        ge=1,
        description="Maximum number of tokens to generate"
    )
    temperature: float = Field(
        default=0.7,
        ge=0.0,
        le=2.0,
        description="Controls randomness (0.0-2.0)"
    )
    system_prompt: Optional[str] = Field(
        default="Kamu adalah Aika, mental health chatbot Universitas Gadjah Mada yang bertindak sebagai teman bicara. Kamu memiliki latar belakang pendidikan psikologi dan pengalaman dalam membantu orang dengan masalah kesehatan mental. Kamu tidak memberikan diagnosis medis, tetapi memberikan dukungan emosional dan saran berdasarkan pengalamanmu.",
        description="Optional: A system prompt to guide the AI's behavior."
    )

    # Add validation if needed
    @validator('history')
    def check_history_format(cls, v):
        if not v:
            raise ValueError("History cannot be empty")
        if v[-1].get('role') != 'user':
            raise ValueError("History must end with a 'user' message")
        # Add more checks if necessary (e.g., alternating roles)
        return v

# --- Response Body Model ---
class ChatResponse(BaseModel):
    response: str = Field(..., description="The generated response from the LLM")
    provider_used: str = Field(..., description="The LLM provider that generated the response")
    model_used: str = Field(..., description="The specific model that generated the response")
    history: List[Dict[str, str]] = Field(..., description="The updated conversation history")

# --- Helper Function for User Lookup/Creation ---
def get_or_create_user(db: Session, identifier: str) -> User:
    """Finds a user by hashed identifier or creates a new one."""
    hashed_id = hashlib.sha256(identifier.encode()).hexdigest()
    user = db.query(User).filter(User.hashed_identifier == hashed_id).first()
    if not user:
        # User not found, create a new one
        llm.logger.info(f"Creating new user record for identifier hash: {hashed_id[:8]}...") # Log first 8 chars of hash for privacy
        user = User()
        user.set_hashed_identifier(identifier) # Use the method to hash and set
        # You might want to populate other fields like email if identifier is email
        # if "@" in identifier: # Basic check
        #    user.email = identifier
        db.add(user)
        try:
            db.commit()
            db.refresh(user)
            llm.logger.info(f"Successfully created user with DB ID: {user.id}")
        except Exception as e:
            db.rollback()
            llm.logger.error(f"Database error creating user: {e}", exc_info=True)
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Could not create user record.")
    return user

# --- API Endpoint (Async) ---
# Add dependencies=[Depends(get_current_user)] if you have authentication
@router.post("/chat", response_model=ChatResponse)
async def handle_chat_request(request: ChatRequest = Body(...)):
    """
    Receives conversation history and returns a response generated by the specified LLM provider.
    """
    if not request.history or request.history[-1].get("role") != "user":
        raise HTTPException(
            status_code=400,
            detail="History must not be empty and must end with a 'user' message."
        )

    try:
        # Call the async unified generate_response function from llm.py
        generated_text = await llm.generate_response(
            history=request.history,
            provider=request.provider,
            model=request.model,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            system_prompt=request.system_prompt
        )

        # Determine the model used (handle defaults)
        actual_model_used = request.model or llm.DEFAULT_PROVIDERS.get(request.provider, "unknown")

        # Check if the response indicates an error from the LLM layer
        if generated_text.startswith("Error:"):
            # Log the error on the server side
            llm.logger.error(f"LLM generation failed for provider {request.provider}: {generated_text}")
            # Return a user-friendly error, but use 500 for backend/API issues
            # You might differentiate: 4xx for input/config errors, 5xx for API failures
            status_code = 400 if "API key not configured" in generated_text or "Invalid history" in generated_text else 503 # 503 Service Unavailable
            raise HTTPException(status_code=status_code, detail=generated_text)

        # Append the AI's response to the history
        updated_history = request.history + [{"role": "assistant", "content": generated_text}]

        return ChatResponse(
            response=generated_text,
            provider_used=request.provider,
            model_used=actual_model_used,
            history=updated_history # Return the updated history
        )

    # Handle potential validation errors from Pydantic (though FastAPI usually does this)
    except ValueError as e:
        llm.logger.warning(f"Value error in /chat endpoint: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    # Catch-all for other unexpected errors during the request handling
    except Exception as e:
        llm.logger.error(f"Unhandled exception in /chat endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An internal server error occurred.")

# Make sure this router is included in your main FastAPI app (e.g., in main.py)
# from app.routes import chat
# app.include_router(chat.router, prefix="/api/v1", tags=["Chat"]) # Adjust prefix as needed