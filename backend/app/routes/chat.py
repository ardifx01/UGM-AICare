# backend/app/routes/chat.py

from sqlalchemy.orm import Session # Import Session
import hashlib # Import hashlib
from fastapi import APIRouter, HTTPException, Body, Depends, status # type: ignore # Added Depends
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Optional, Literal
from datetime import datetime # Import datetime

# Adjust import based on your project structure
from app.database import get_db
from app.models import User, Conversation # Import User and Conversation models
from app.core import llm
from app.core.llm import LLMProvider # Import the type hint

# Import your authentication dependency if you have one
# from app.dependencies import get_current_user # Example

router = APIRouter()

# --- Request Body Model ---
class ChatRequest(BaseModel):
    user_identifier: str = Field(..., description="Unique identifier for the user (e.g., email, username, or frontend-generated ID). Will be hashed.")
    session_id: str = Field(..., description="Unique identifier for this specific conversation session (e.g., UUID generated by frontend).")
    history: List[Dict[str, str]] = Field(..., description="Conversation history, ending with the user's prompt")
    provider: LLMProvider = Field(
        default="togetherai",
        description="The LLM provider to use ('togetherai' or 'gemini')"
    )
    model: Optional[str] = Field(
        default=None,
        description="Optional: Specify a model name override"
    )
    max_tokens: int = Field(
        default=1024,
        ge=1,
        description="Maximum number of tokens to generate"
    )
    temperature: float = Field(
        default=0.7,
        ge=0.0,
        le=2.0,
        description="Controls randomness (0.0-2.0)"
    )
    system_prompt: Optional[str] = Field(
        default="Kamu adalah Aika, mental health chatbot Universitas Gadjah Mada yang bertindak sebagai teman bicara. Kamu memiliki latar belakang pendidikan psikologi dan pengalaman dalam membantu orang dengan masalah kesehatan mental. Kamu tidak memberikan diagnosis medis, tetapi memberikan dukungan emosional dan saran berdasarkan pengalamanmu.",
        description="Optional: A system prompt to guide the AI's behavior."
    )

    # Add validation if needed
    @validator('history')
    def check_history_format(cls, v):
        if not v:
            raise ValueError("History cannot be empty")
        if v[-1].get('role') != 'user':
            raise ValueError("History must end with a 'user' message")
        # Add more checks if necessary (e.g., alternating roles)
        return v

# --- Response Body Model ---
class ChatResponse(BaseModel):
    response: str = Field(..., description="The generated response from the LLM")
    provider_used: str = Field(..., description="The LLM provider that generated the response")
    model_used: str = Field(..., description="The specific model that generated the response")
    history: List[Dict[str, str]] = Field(..., description="The updated conversation history")

# Simplified backend get_or_create_user - now receives the already-hashed ID
def get_or_create_user(db: Session, received_hashed_identifier: str) -> User:
    """Finds a user by hashed identifier or creates a new one."""
    user = db.query(User).filter(User.hashed_identifier == received_hashed_identifier).first()
    if not user:
        # User not found, create a new one
        llm.logger.info(f"Creating new user record for identifier hash: {received_hashed_identifier[:8]}...") 
        user = User(hashed_identifier=received_hashed_identifier) # Store the hash directly
        # Potentially fetch email/name from Google token *if* frontend sends it securely, 
        # but avoid storing if not necessary for functionality.
        db.add(user)
        try:
            db.commit()
            db.refresh(user)
            llm.logger.info(f"Successfully created user with DB ID: {user.id}")
        except Exception as e:
            db.rollback()
            llm.logger.error(f"Database error creating user: {e}", exc_info=True)
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Could not create user record.")
    return user

# --- API Endpoint (Async) ---
# Add dependencies=[Depends(get_current_user)] if you have authentication
@router.post("/chat", response_model=ChatResponse)
async def handle_chat_request(
    request: ChatRequest = Body(...)
    , db: Session = Depends(get_db) # Use the database session dependency
    ):
    """
    Receives conversation history, user/session IDs, saves the interaction,
    and returns a response generated by the specified LLM provider.
    """
    try:
        # 1. Get or Create User
        # Use the provided identifier to find/create the user record
        # Wrap DB operations potentially requiring commit in a try/except
        try:
            db_user = get_or_create_user(db, request.user_identifier)
        except HTTPException as http_exc: # Catch specific exception from helper
            raise http_exc
        except Exception as e:
            llm.logger.error(f"Failed during user lookup/creation: {e}", exc_info=True)
            raise HTTPException(status_code=500, detail="Error processing user information.")

        # Extract the user message from the history
        user_message_content = request.history[-1]['content']

        # 2. Call the LLM to get a response
        generated_text = await llm.generate_response(
            history=request.history,
            provider=request.provider,
            model=request.model,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            system_prompt=request.system_prompt
        )

        # Check for errors from LLM generation
        if generated_text.startswith("Error:"):
            llm.logger.error(f"LLM generation failed for provider {request.provider}: {generated_text}")
            status_code = 400 if "API key" in generated_text or "Invalid history" in generated_text else 503
            raise HTTPException(status_code=status_code, detail=generated_text)

        # 3. Save the conversation turn to the database
        try:
            conversation_entry = Conversation(
                user_id=db_user.id, # Use the integer primary key from the User table
                session_id=request.session_id,
                message=user_message_content,
                response=generated_text,
                timestamp=datetime.now() # Or use DB default if configured
            )
            db.add(conversation_entry)
            db.commit()
            llm.logger.info(f"Saved conversation turn for user DB ID {db_user.id}, session ID {request.session_id}")
        except Exception as e:
            db.rollback()
            llm.logger.error(f"Database error saving conversation: {e}", exc_info=True)
            # Decide if you should still return the response to the user even if saving failed
            # For now, we'll raise an error, but you might want to just log it
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Could not save conversation to database.")

        # 4. Prepare and return the response
        actual_model_used = request.model or llm.DEFAULT_PROVIDERS.get(request.provider, "unknown")
        updated_history = request.history + [{"role": "assistant", "content": generated_text}]

        # Optional: Update Redis cache if still using it for short-term context
        # from app.core.memory import AikaMemory
        # AikaMemory.save_memory_direct(request.session_id, updated_history) # Use session_id as key

        return ChatResponse(
            response=generated_text,
            provider_used=request.provider,
            model_used=actual_model_used,
            history=updated_history # Return updated history for frontend state
        )

    # Handle potential validation errors from Pydantic
    except ValueError as e:
        llm.logger.warning(f"Value error in /chat endpoint: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    # Catch-all for other unexpected errors
    except Exception as e:
        llm.logger.error(f"Unhandled exception in /chat endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An internal server error occurred.")

    # Handle potential validation errors from Pydantic (though FastAPI usually does this)
    except ValueError as e:
        llm.logger.warning(f"Value error in /chat endpoint: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    # Catch-all for other unexpected errors during the request handling
    except Exception as e:
        llm.logger.error(f"Unhandled exception in /chat endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An internal server error occurred.")