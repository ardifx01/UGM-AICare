# backend/app/routes/chat.py

from fastapi import APIRouter, HTTPException, Body, Depends # Added Depends
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Literal

# Adjust import based on your project structure
# Assuming llm.py is in app/core/llm.py and chat.py is in app/routes/chat.py
from app.core import llm
from app.core.llm import LLMProvider # Import the type hint

# Import your authentication dependency if you have one
# from app.dependencies import get_current_user # Example

router = APIRouter()

# --- Request Body Model ---
class ChatRequest(BaseModel):
    history: List[Dict[str, str]] = Field(..., description="Conversation history, ending with the user's prompt")
    provider: LLMProvider = Field( # Use the Literal type
        default="togetherai",
        description="The LLM provider to use ('togetherai' or 'gemini')"
    )
    model: Optional[str] = Field(
        default=None,
        description="Optional: Specify a model name override"
    )
    max_tokens: int = Field(
        default=1024, # Increased default max_tokens
        ge=1, # Ensure positive value
        description="Maximum number of tokens to generate"
    )
    temperature: float = Field(
        default=0.7,
        ge=0.0, # Ensure non-negative
        le=2.0, # Allow higher temps if needed
        description="Controls randomness (0.0-2.0)"
    )
    system_prompt: Optional[str] = Field(
        default="You are AICare, a friendly and helpful AI assistant developed by students at UGM. Provide concise and supportive responses.", # Example system prompt
        description="Optional: A system prompt to guide the AI's behavior."
    )
    # Add other parameters if needed

# --- Response Body Model ---
class ChatResponse(BaseModel):
    response: str = Field(..., description="The generated response from the LLM")
    provider_used: str = Field(..., description="The LLM provider that generated the response")
    model_used: str = Field(..., description="The specific model that generated the response")
    history: List[Dict[str, str]] = Field(..., description="The updated conversation history")


# --- API Endpoint (Async) ---
# Add dependencies=[Depends(get_current_user)] if you have authentication
@router.post("/chat", response_model=ChatResponse)
async def handle_chat_request(request: ChatRequest = Body(...)):
    """
    Receives conversation history and returns a response generated by the specified LLM provider.
    """
    if not request.history or request.history[-1].get("role") != "user":
        raise HTTPException(
            status_code=400,
            detail="History must not be empty and must end with a 'user' message."
        )

    try:
        # Call the async unified generate_response function from llm.py
        generated_text = await llm.generate_response(
            history=request.history,
            provider=request.provider,
            model=request.model,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            system_prompt=request.system_prompt
        )

        # Determine the model used (handle defaults)
        actual_model_used = request.model or llm.DEFAULT_PROVIDERS.get(request.provider, "unknown")

        # Check if the response indicates an error from the LLM layer
        if generated_text.startswith("Error:"):
            # Log the error on the server side
            llm.logger.error(f"LLM generation failed for provider {request.provider}: {generated_text}")
            # Return a user-friendly error, but use 500 for backend/API issues
            # You might differentiate: 4xx for input/config errors, 5xx for API failures
            status_code = 400 if "API key not configured" in generated_text or "Invalid history" in generated_text else 503 # 503 Service Unavailable
            raise HTTPException(status_code=status_code, detail=generated_text)

        # Append the AI's response to the history
        updated_history = request.history + [{"role": "assistant", "content": generated_text}]

        return ChatResponse(
            response=generated_text,
            provider_used=request.provider,
            model_used=actual_model_used,
            history=updated_history # Return the updated history
        )

    # Handle potential validation errors from Pydantic (though FastAPI usually does this)
    except ValueError as e:
        llm.logger.warning(f"Value error in /chat endpoint: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    # Catch-all for other unexpected errors during the request handling
    except Exception as e:
        llm.logger.error(f"Unhandled exception in /chat endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An internal server error occurred.")

# Make sure this router is included in your main FastAPI app (e.g., in main.py)
# from app.routes import chat
# app.include_router(chat.router, prefix="/api/v1", tags=["Chat"]) # Adjust prefix as needed