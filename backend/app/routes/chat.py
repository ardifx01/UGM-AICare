# backend/app/routes/chat.py

from sqlalchemy.orm import Session # Import Session
from sqlalchemy import desc
import hashlib # Import hashlib
from fastapi import APIRouter, HTTPException, Body, Depends, status, BackgroundTasks # type: ignore # Added Depends
from pydantic import BaseModel, Field, validator
from typing import List, Dict, Optional, Literal
from datetime import datetime # Import datetime

# Adjust import based on your project structure
from app.database import get_db
from app.models import User, Conversation, UserSummary # Import User and Conversation models
from app.core import llm
from app.core.llm import LLMProvider # Import the type hint

# Import your authentication dependency if you have one
# from app.dependencies import get_current_user # Example

router = APIRouter()

# --- Request Body Model ---
class ChatRequest(BaseModel):
    user_identifier: str = Field(..., description="Unique identifier for the user (e.g., email, username, or frontend-generated ID). Will be hashed.")
    session_id: str = Field(..., description="Unique identifier for this specific conversation session (e.g., UUID generated by frontend).")
    history: List[Dict[str, str]] = Field(..., description="Conversation history, ending with the user's prompt")
    provider: LLMProvider = Field(
        default="togetherai",
        description="The LLM provider to use ('togetherai' or 'gemini')"
    )
    model: Optional[str] = Field(
        default=None,
        description="Optional: Specify a model name override"
    )
    max_tokens: int = Field(
        default=1024,
        ge=1,
        description="Maximum number of tokens to generate"
    )
    temperature: float = Field(
        default=0.7,
        ge=0.0,
        le=2.0,
        description="Controls randomness (0.0-2.0)"
    )
    system_prompt: Optional[str] = Field(
        default="Kamu adalah Aika, mental health chatbot Universitas Gadjah Mada yang bertindak sebagai teman bicara. Kamu memiliki latar belakang pendidikan psikologi dan pengalaman dalam membantu orang dengan masalah kesehatan mental. Kamu tidak memberikan diagnosis medis, tetapi memberikan dukungan emosional dan saran berdasarkan pengalamanmu.",
        description="Optional: A system prompt to guide the AI's behavior."
    )

    # Add validation if needed
    @validator('history')
    def check_history_format(cls, v):
        if not v:
            raise ValueError("History cannot be empty")
        if v[-1].get('role') != 'user':
            raise ValueError("History must end with a 'user' message")
        # Add more checks if necessary (e.g., alternating roles)
        return v

# --- Response Body Model ---
class ChatResponse(BaseModel):
    response: str = Field(..., description="The generated response from the LLM")
    provider_used: str = Field(..., description="The LLM provider that generated the response")
    model_used: str = Field(..., description="The specific model that generated the response")
    history: List[Dict[str, str]] = Field(..., description="The updated conversation history")

# Simplified backend get_or_create_user - now receives the already-hashed ID
def get_or_create_user(db: Session, received_hashed_identifier: str) -> User:
    """Finds a user by hashed identifier or creates a new one."""
    user = db.query(User).filter(User.hashed_identifier == received_hashed_identifier).first()
    if not user:
        # User not found, create a new one
        llm.logger.info(f"Creating new user record for identifier hash: {received_hashed_identifier[:8]}...") 
        user = User(hashed_identifier=received_hashed_identifier) # Store the hash directly
        # Potentially fetch email/name from Google token *if* frontend sends it securely, 
        # but avoid storing if not necessary for functionality.
        db.add(user)
        try:
            db.commit()
            db.refresh(user)
            llm.logger.info(f"Successfully created user with DB ID: {user.id}")
        except Exception as e:
            db.rollback()
            llm.logger.error(f"Database error creating user: {e}", exc_info=True)
            raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Could not create user record.")
    return user

# --- API Endpoint (Async) ---
# Add dependencies=[Depends(get_current_user)] if you have authentication
@router.post("/chat", response_model=ChatResponse)
async def handle_chat_request(
    request: ChatRequest = Body(...),
    background_tasks: BackgroundTasks = BackgroundTasks(), # For background tasks
    db: Session = Depends(get_db) # Use the database session dependency
    ):
    """
    Handles chat, detects session changes to trigger summarization of the *previous* session,
    and injects the *latest available* summary into the context for the *current* session.
    """
    try:
        # 1. Get or Create User (using the hashed identifier)
        try:
            # Assuming request.user_identifier contains the HASH from frontend
            db_user = get_or_create_user(db, request.user_identifier) 
        except Exception as e:
             llm.logger.error(f"Failed during user lookup/creation: {e}", exc_info=True)
             raise HTTPException(status_code=500, detail="Error processing user information.")

        user_message_content = request.history[-1]['content']
        current_session_id = request.session_id

        # 2. Detect Session Change and Trigger Previous Summary (if applicable)
        previous_session_id_to_summarize = None
        latest_message_from_user = db.query(Conversation)\
            .filter(Conversation.user_id == db_user.id)\
            .order_by(Conversation.timestamp.desc())\
            .first()

        if latest_message_from_user and latest_message_from_user.session_id != current_session_id:
            # This is a new session (or the very first message ever for the user)
            # We need to summarize the *previous* session
            previous_session_id_to_summarize = latest_message_from_user.session_id
            llm.logger.info(f"New session detected ({current_session_id}) for user {db_user.id}. Previous session was {previous_session_id_to_summarize}.")
            # Run summarization in the background to avoid delaying the current chat response
            background_tasks.add_task(summarize_and_save, db, db_user.id, previous_session_id_to_summarize)
        elif not latest_message_from_user:
             llm.logger.info(f"First ever message for user {db_user.id} in session {current_session_id}.")
             # No previous session to summarize
        # else: it's the same session, do nothing regarding summarization trigger


        # 3. Prepare History for LLM - Inject *Latest Available* Summary
        history_for_llm = request.history # Start with history from current request

        # Fetch the most recent summary for this user, regardless of which session it's from
        latest_summary = db.query(UserSummary)\
            .filter(UserSummary.user_id == db_user.id)\
            .order_by(UserSummary.timestamp.desc())\
            .first()

        if latest_summary:
            summary_injection = {
                "role": "system", 
                "content": f"Reminder of key points from a previous conversation (summarized on {latest_summary.timestamp.strftime('%Y-%m-%d')}): {latest_summary.summary_text}"
            }
            # Prepend the summary. Consider if this should only happen on the *first* message of a new session.
            # For simplicity now, let's prepend it always if a summary exists.
            history_for_llm = [summary_injection] + history_for_llm
            llm.logger.info(f"Injected previous summary for user {db_user.id} into LLM context.")

        # 4. Call the LLM to get a response for the CURRENT message
        generated_text = await llm.generate_response(
            history=history_for_llm, # Use history potentially including the summary
            provider=request.provider,
            model=request.model,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            system_prompt=request.system_prompt # The main system prompt still applies
        )

        # Check for errors from LLM generation
        if generated_text.startswith("Error:"):
            llm.logger.error(f"LLM generation failed for provider {request.provider}: {generated_text}")
            status_code = 400 if "API key" in generated_text or "Invalid history" in generated_text else 503
            raise HTTPException(status_code=status_code, detail=generated_text)

        # 5. Save the CURRENT conversation turn to the database
        try:
            conversation_entry = Conversation(
                user_id=db_user.id, 
                session_id=current_session_id, # Use the current session ID
                message=user_message_content,
                response=generated_text,
                timestamp=datetime.now() 
            )
            db.add(conversation_entry)
            db.commit()
        except Exception as e:
            # ... (handle DB save error, rollback) ...
            raise HTTPException(status_code=500, detail="Could not save current conversation turn.")

        # 6. Prepare and return the response
        actual_model_used = request.model or llm.DEFAULT_PROVIDERS.get(request.provider, "unknown")
        # The history returned to frontend should ideally NOT include the injected summary, just the actual chat turns
        updated_history_for_frontend = request.history + [{"role": "assistant", "content": generated_text}]


        return ChatResponse(
            response=generated_text,
            provider_used=request.provider,
            model_used=actual_model_used,
            history=updated_history_for_frontend 
        )

    # Handle potential validation errors from Pydantic
    except ValueError as e:
        llm.logger.warning(f"Value error in /chat endpoint: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    # Catch-all for other unexpected errors
    except Exception as e:
        llm.logger.error(f"Unhandled exception in /chat endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An internal server error occurred.")

    # Handle potential validation errors from Pydantic (though FastAPI usually does this)
    except ValueError as e:
        llm.logger.warning(f"Value error in /chat endpoint: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    # Catch-all for other unexpected errors during the request handling
    except Exception as e:
        llm.logger.error(f"Unhandled exception in /chat endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="An internal server error occurred.")
    
class SummarizeRequest(BaseModel):
    session_id: str 
    user_identifier: str # To find the user_id

# --- Define the Summarization Function (can be in this file or a helper module) ---
async def summarize_and_save(db: Session, user_id: int, session_id_to_summarize: str):
    """Fetches history, calls LLM to summarize, and saves to UserSummary table."""
    llm.logger.info(f"Background Task: Starting summarization for user {user_id}, session {session_id_to_summarize}")
    try:
        # 1. Retrieve conversation history for the session to summarize
        conversation_history = db.query(Conversation)\
            .filter(Conversation.session_id == session_id_to_summarize, Conversation.user_id == user_id)\
            .order_by(Conversation.timestamp.asc())\
            .all()

        if not conversation_history or len(conversation_history) < 2: # Don't summarize very short/empty chats
             llm.logger.info(f"Background Task: Skipping summarization for session {session_id_to_summarize} (too short/no history).")
             return

        # 2. Format history for LLM
        # Consider filtering out previous summaries if they were injected as system messages
        formatted_history = "\n".join([f"{msg.role}: {msg.message if msg.role == 'user' else msg.response}" for msg in conversation_history])

        # 3. Create the summarization prompt
        summarization_prompt = f"""Please summarize the key points, user's expressed feelings, and main topics discussed in the following conversation history. Focus on aspects relevant to mental well-being for UGM-AICare users. Be concise.

Conversation:
{formatted_history}

Summary:"""

        # 4. Call the LLM
        summary_llm_history = [{"role": "user", "content": summarization_prompt}]
        summary_text = await llm.generate_response(
             history=summary_llm_history, 
             provider="gemini", # Or your preferred model
             max_tokens=250, 
             temperature=0.5 
        )

        if summary_text.startswith("Error:"):
             raise Exception(f"LLM Error: {summary_text}")

        # 5. Save the summary
        # Check if a summary for this exact user/session already exists to prevent duplicates (optional)
        existing_summary = db.query(UserSummary).filter(UserSummary.user_id == user_id, UserSummary.summary_text.like(f"%Session {session_id_to_summarize}%")).first() # Example check
        if existing_summary:
             llm.logger.info(f"Background Task: Summary already likely exists for session {session_id_to_summarize}. Skipping save.")
             return

        new_summary = UserSummary(
            user_id=user_id,
            # Add session ID to summary text for context? Optional.
            summary_text=f"(Summary for session {session_id_to_summarize}): {summary_text.strip()}" 
        )
        db.add(new_summary)
        db.commit() # Commit within the background task requires careful session management or a separate session
        llm.logger.info(f"Background Task: Saved summary for user {user_id}, session {session_id_to_summarize}")

    except Exception as e:
        db.rollback() # Rollback on error
        llm.logger.error(f"Background Task: Failed to summarize session {session_id_to_summarize} for user {user_id}: {e}", exc_info=True)
    finally:
        db.close() # Ensure the session used by the background task is closed